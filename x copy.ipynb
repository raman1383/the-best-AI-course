{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get parameter count & break down by layer\n",
    "# TODO: get VRAM usage for empty, full seq, grad states\n",
    "\n",
    "# TODO: train standard Transformer on entropy dataset\n",
    "# TODO: plot loss and weight checkpoints\n",
    "# TODO: then introduce RL\n",
    "\n",
    "# TODO: get first 30 MB of tiny-stories dataset\n",
    "\n",
    "# TODO(opt): use tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. No GPU found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get free and total memory in bytes\n",
    "    free_memory_bytes, total_memory_bytes = torch.cuda.mem_get_info()\n",
    "\n",
    "    # Convert to GB for better readability\n",
    "    free_memory_gb = free_memory_bytes / (1024**3)\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    print(f\"Total GPU VRAM: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"Free GPU VRAM: {free_memory_gb:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478947894789478947894789478947894789478947894789478947894789478947894789478947894789478947894789478\n",
      "145914591459145914591459145914591459145914591459145914591459145914591459145914591459145914591459145\n",
      "025602560\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "with open('basic_entropy_dataset_2.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(text[:209])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "context_length = 100     # Max input length\n",
    "batch_size = 32         # how many examples in one forward pass\n",
    "embedding_dim = 12     # len of each vector representing a token\n",
    "num_heads = 4           # number of attention heads in each layer\n",
    "num_layers = 3          # number of transformer blocks(each = attention+MLP)\n",
    "dropout = 0.1\n",
    "num_epochs = 10         # how many times go through the entire dataset\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "vocab size:  11\n",
      "23456789\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# Encoding functions\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"\\n0123456789\"))\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "print(decode([3, 4, 5, 6, 7, 8, 9, 10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9,\n",
      "         3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9,\n",
      "         3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9]])\n",
      "Y: tensor([[5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3,\n",
      "         5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3,\n",
      "         5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3]])\n",
      "X- : tensor([3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9,\n",
      "        3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9,\n",
      "        3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9])\n",
      "Y- : tensor([[5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3,\n",
      "         5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3,\n",
      "         5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3, 5, 6, 9, 3]])\n",
      "X shape: torch.Size([1, 64])\n",
      "Y shape: torch.Size([1, 64])\n",
      "X: 2458245824582458245824582458245824582458245824582458245824582458\n",
      "Y: 4582458245824582458245824582458245824582458245824582458245824582\n"
     ]
    }
   ],
   "source": [
    "# Convert the full dataset (which is a string of characters) into a tensor of integers(idx of chars in vocab)\n",
    "# e.g: \"hello\" → [7, 4, 11, 11, 14] → tensor([7, 4, 11, 11, 14])\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "\n",
    "# get a line, give each char positional encoding, then sample \n",
    "\n",
    "def get_batch(split='train'):\n",
    "    # Randomly samples batch_size starting indices (integers) from 0 to (len(data)-context_length)\n",
    "    # Each index is the start of a training sequence of length context_length.\n",
    "    # Subtracting context_length ensures you don’t run past the end of the data when slicing.\n",
    "    example_seeds = torch.randint(0,100, (1,))\n",
    "\n",
    "    x = torch.stack([data[(i*100):(i*100)+context_length] for i in example_seeds])\n",
    "    y = torch.stack([data[(i*100)+1:(i*100)+context_length+1] for i in example_seeds])\n",
    "\n",
    "    print(\"X:\", x)\n",
    "    print(\"Y:\", y)\n",
    "    return x, y\n",
    "    # return x.to(device), y.to(device) #Given: \"the ca\", Predict: \"he cat\"\n",
    "\n",
    "x= get_batch()\n",
    "\n",
    "print(\"X- :\", x[0][0])\n",
    "print(\"Y- :\", x[1])\n",
    "\n",
    "\n",
    "print(\"X shape:\", x[0].shape)\n",
    "print(\"Y shape:\", x[1].shape)\n",
    "\n",
    "print(\"X:\", decode(x[0][0].tolist()))\n",
    "print(\"Y:\", decode(x[1][0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "\n",
    "    # embedding_dim: input dimension for each token (e.g 128)\n",
    "    # head_size: output dimension of this head (e.g., 128 // num_heads_being_4 = 32)\n",
    "    def __init__(self, embedding_dim, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize single-layer fully-connected neural network\n",
    "        #                     in_features ,  out_features  , bias=false\n",
    "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "\n",
    "        # Creates a lower triangular matrix of shape [context_length, context_length]\n",
    "        # Used for causal masking: tokens can only attend to earlier or same-position tokens, never future tokens\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length))) \n",
    "\n",
    "        # Dropout layer for regularizing attention weights, prevents overfitting.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, time steps (context length), channels (embedding_dim)\n",
    "\n",
    "        # shape [B, T, head_size]\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # Computes attention scores: dot product between each query and all keys.\n",
    "        # Shape becomes [B, T, T]: every token attends to every other token\n",
    "        wei = q @ k.transpose(-2, -1) / (C ** 0.5) # Divide by sqrt(C) to stabilize gradients\n",
    "        \n",
    "        # Apply the causal mask: upper triangle is -inf → zero probability after softmax\n",
    "        # so that position t can’t see future positions t+1 and beyond.\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "\n",
    "        # Turns the attention scores into a probability distribution over past tokens\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # Dropout on attention weights (regularization).\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Compute the weighted average of the values for each position\n",
    "        v = self.value(x)\n",
    "        return wei @ v # Output shape is [B, T, head_size]\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # Splits the full embedding into multiple smaller \"heads\" that run attention independently. \n",
    "        # This allows the model to learn multiple types of relationships in parallel\n",
    "        head_size = embedding_dim // num_heads\n",
    "\n",
    "        # Creates num_heads attention heads. \n",
    "        # Each head processes the input differently with its own Q, K, V weight matrices.\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(embedding_dim, head_size) for _ in range(num_heads)])\n",
    "\n",
    "        # After heads are concatenated, project back to the original embedding dimension. \n",
    "        # This mixes information across heads.\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # Dropout after projection — again, to regularize.\n",
    "        self.dropout = nn.Dropout(dropout)    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Run all heads independently and concatenate their outputs along the feature dimension.\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # Apply the projection + dropout to mix and regularize the combined multi-head output.\n",
    "        return self.dropout(self.proj(out))\n",
    "    \n",
    "\n",
    "\n",
    "# A standard 2-layer MLP(Input → expand → ReLU → compress → Dropout).\n",
    "# Applies to each token independently (position-wise).\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  Applies layer norm, then attention. Adds the result back to x → residual connection.   \n",
    "        x = x + self.sa(self.ln1(x))\n",
    "\n",
    "        # Same idea: norm → feedforward → residual\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        # Long-range dependencies (via attention)\n",
    "        # Complex feature transformations (via MLP)\n",
    "        # Stable, deep learning (via residuals + norm)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is your complete transformer model(input embeddings(token+pos), 3 transformer blocks, output layer)\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # This allows tokens to be transformed into dense vectors the model can work with.\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #TODO: RoPE\n",
    "        # Adds information about token position in the sequence, attention has no sense of order ! \n",
    "        # Each position 0 to context_length-1 gets a learnable vector of same size as token embedding.\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "\n",
    "        # Builds a stack of num_layers decoder blocks. \n",
    "        # Each block includes multi-head attention, feedforward, layer norm, and residuals\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            DecoderBlock(embedding_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer normalization after all blocks.\n",
    "        # Helps stabilize activations and gradients before output.\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Maps the output of each token (which is a vector) to logits over the vocabulary.\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "    # idx: input token IDs of shape [B, T] (batch size × context length)\n",
    "    # targets: next-token targets (same shape), optional (used for inference)\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape # B: batch size,  T: time steps (context length)\n",
    "\n",
    "\n",
    "        # Shape becomes [B, T, embedding_dim]\n",
    "        tok_emb = self.token_embedding(idx) #  Each token is mapped to its learned vector\n",
    "\n",
    "        # Shape: [T, embedding_dim]\n",
    "        # Each position 0...T-1 gets a learnable vector\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device)) \n",
    "        # Broadcasted to match batch size\n",
    "\n",
    "        # Shape: [B, T, embedding_dim]\n",
    "        # Add both embeddings together elementwise, each token has a sense of identity and order\n",
    "        x = tok_emb + pos_emb \n",
    "\n",
    "\n",
    "        # Passes the full sequence through the stack of decoder blocks(3 in this case)\n",
    "        # Each block refines the representation of each token\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply layer normalization at the end for stability.\n",
    "        # Improves convergence and output quality\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Shape: [B, T, vocab_size]\n",
    "        # For each position in the sequence, you get a score for each possible next token.\n",
    "        logits = self.head(x) \n",
    "\n",
    "\n",
    "        # If no labels are provided, just return the logits (e.g., for sampling).\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "\n",
    "        # Flatten logits and targets: from [B, T, vocab_size] and [B, T] to [B*T, ...]\n",
    "        # Compute cross-entropy loss\n",
    "        # Teaches the model to assign high probabilities to the correct next characters.\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        # Return both predictions and the loss (for backpropagation during training).\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    # idx: a starting sequence of token IDs (e.g., [[12]])\n",
    "    # max_new_tokens: how many new tokens to append\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        # Loop to generate one token at a time, up to max_new_tokens.\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # The model can only handle inputs up to that length, so\n",
    "            # Truncate input to most recent context_length tokens\n",
    "            idx_cond = idx[:, -context_length:]\n",
    "\n",
    "            # Pass current context into the model, no loss just logits\n",
    "            logits, _ = self(idx_cond)\n",
    "\n",
    "            # Take the logits for the last position in the sequence.\n",
    "            # Softmax turns it into a probability distribution over the vocabulary.\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "            # Sample from the probability distribution. + a little randomness for creativity \n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Add the sampled token to the end of the input.\n",
    "            # The loop continues with the longer sequence.\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "        # The final sequence includes the original input + all generated tokens.\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 0 | Loss: 4.2929\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 1 | Loss: 3.9854\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 2 | Loss: 3.7052\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 3 | Loss: 3.5470\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 4 | Loss: 3.5303\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 5 | Loss: 3.3733\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 6 | Loss: 3.3404\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 7 | Loss: 3.3467\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 8 | Loss: 3.2707\n",
      "torch.Size([32, 64, 65])\n",
      "torch.Size([])\n",
      "Epoch 9 | Loss: 3.3032\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    🧠 The model gets a random chunk of training data.\n",
    "\n",
    "    🎯 It tries to predict the next token in each sequence.\n",
    "\n",
    "    ❌ It measures how wrong it was (loss).\n",
    "\n",
    "    📉 It computes how to adjust weights (gradients).\n",
    "\n",
    "    🔧 It updates the weights slightly to improve.\n",
    "\n",
    "    📈 Over time, the model becomes better at next-token prediction.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creates an instance of your Transformer model and moves it to the GPU (or CPU) based on availability\n",
    "# All your tensors (x, y) and model parameters must\n",
    "#   live on the same device (GPU if available). to(device) ensures that\n",
    "model = TransformerLanguageModel().to(device)\n",
    "\n",
    "# Creates the AdamW optimizer (Adam + weight decay) for updating model parameters.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Repeats the training process num_epochs times (complete passes over the data).\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Some layers like Dropout and LayerNorm behave differently \n",
    "    #   in training vs inference (e.g., dropout is disabled in .eval() mode).\n",
    "    model.train()\n",
    "\n",
    "    # Grabs one batch of training examples\n",
    "    #   xb = input tensor (token sequences)\n",
    "    #   yb = target tensor (the correct next characters)\n",
    "    xb, yb = get_batch()\n",
    "\n",
    "    # Pass the inputs (xb) through the model to get:\n",
    "    #   logits: the raw predicted scores (before softmax)\n",
    "    #   loss: cross-entropy loss comparing logits to yb\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    print(logits.shape) # [batch_size, context_length, vocab_size]\n",
    "    print(loss.shape)\n",
    "\n",
    "    # Gradients accumulate by default in PyTorch. If you don’t\n",
    "    #   clear them, you’ll be mixing gradients from multiple steps \n",
    "    #   which leads to incorrect updates\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # This is automatic differentiation. PyTorch builds a computation graph as you go,\n",
    "    #    and this line applies backpropagation\n",
    "    # Computes the gradients of the loss w.r.t. all model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Updates the model’s parameters using the gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Logs the current epoch number and the loss value. for monitoring\n",
    "    print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLanguageModel(\n",
       "  (token_embedding): Embedding(65, 128)\n",
       "  (position_embedding): Embedding(64, 128)\n",
       "  (blocks): Sequential(\n",
       "    (0): DecoderBlock(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SelfAttentionHead(\n",
       "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SelfAttentionHead(\n",
       "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SelfAttentionHead(\n",
       "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "What's in the .pt file?\n",
    "    It stores a mapping like:\n",
    "\n",
    "{\n",
    "  'token_embedding.weight': Tensor([...]),\n",
    "  'blocks.0.sa.heads.0.key.weight': Tensor([...]),\n",
    "  ...\n",
    "}\n",
    "\n",
    "Each tensor is a learned weight or bias from your model's layers.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save weights\n",
    "# model.state_dict() returns a Python dictionary \n",
    "#   that holds all the learned weights (parameters) of your model.\n",
    "# torch.save(...) writes that dictionary to disk as a file.\n",
    "torch.save(model.state_dict(), \"transformer_weights.pt\")\n",
    "\n",
    "\n",
    "# Load weights\n",
    "# Creates a new instance of your model. move it to GPU\n",
    "model = TransformerLanguageModel().to(device)\n",
    "\n",
    "# torch.load(...) reads the .pt file from disk and returns the dictionary of saved weights.\n",
    "# model.load_state_dict(...) copies those weights into your new model instance\n",
    "model.load_state_dict(torch.load(\"transformer_weights.pt\"))\n",
    "\n",
    "# Sets the model to inference mode.\n",
    "# Certain layers behave differently in .train() vs .eval():\n",
    "    #Dropout is disabled (no random masking)\n",
    "    #LayerNorm/BatchNorm uses learned averages instead of batch stats\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hne'bxueoed Dienheltzs\n",
      "eFseDdTnc\n",
      "t ahIuat,np uwernf;th  ne mhr eyhUarus\n",
      "iRFh hi lict,IUUioasnAi Oeo t\n"
     ]
    }
   ],
   "source": [
    "# sample inference\n",
    "\n",
    "# Converts the single character 'H' into its token ID using the character-to-index map stoi.\n",
    "# Wraps it in two layers of brackets: [[...]], making it shape [1, 1], which is:\n",
    "    # Batch size = 1\n",
    "    # Sequence length = 1\n",
    "#       because Transformers expect input as a 2D tensor: [batch_size, context_length]\n",
    "# Converts it into a torch.Tensor of type long (required for indexing into embeddings)\n",
    "# Moves it to the same device as your model (GPU or CPU).\n",
    "context = torch.tensor([[stoi['H']]], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The generate() function:\n",
    "\n",
    "    Truncates the input to the max context length\n",
    "\n",
    "    Gets logits from the model\n",
    "\n",
    "    Converts logits to probabilities\n",
    "\n",
    "    Samples the next token from those probabilities\n",
    "\n",
    "    Appends the token to the sequence\n",
    "\n",
    "    Repeats\n",
    "\"\"\"\n",
    "generated = model.generate(context, max_new_tokens=100)\n",
    "\n",
    "# turns your generated tensor of integers into human-readable text\n",
    "# generated[0]: Selects the first (and only) sequence in the batch.\n",
    "# .tolist(): Converts the tensor (e.g. tensor([34, 17, 53, ...])) to a Python list.\n",
    "# decode(...): Converts list of token IDs back into a string using your itos (index-to-symbol) mapping.\n",
    "print(decode(generated[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
